groups:
{% if 'node' in client.products %}
- name: "{{ client.name }}-node"
  rules:
  - alert: InstanceDown
    annotations:
      description: '{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for
        more than 5 minutes.{% endraw %}'
      summary: "{% raw %}Instance {{ $labels.instance }} down{% endraw %}"
    expr: '{% raw %}up{clientID="{% endraw %}{{ client.name }}{% raw %}"} == 0{% endraw %}'
    for: 5m
    labels:
      severity: critical
  - alert: RebootRequired
    annotations:
      description: "{% raw %}{{ $labels.instance }} requires a reboot.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - reboot required{% endraw %}"
    expr: "{% raw %}node_reboot_required{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0{% endraw %}"
    labels:
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available space left and is filling up.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of space within the next 24 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 40\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 24*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of space within the next 4 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 20\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 4*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available space left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 5% space left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 5\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
    \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
          only {{ printf \"%.2f\" $value }}% available space left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 3% space left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 3\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
    \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of inodes within the next 24 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 40\nand\n  predict_linear(node_filesystem_files_free{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 24*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of inodes within the next 4 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 20\nand\n  predict_linear(node_filesystem_files_free{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 4*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 5% inodes left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 5\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
      \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 3% inodes left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 3\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
      \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeNetworkReceiveErrs
    annotations:
      description: "{% raw %}{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf \"%.0f\" $value }} receive errors in the last two minutes.{% endraw %}"
      summary: "{% raw %}Network interface is reporting many receive errors.{% endraw %}"
    expr: "{% raw %}increase(node_network_receive_errs_total{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[2m]) > 10{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeNetworkTransmitErrs
    annotations:
      description: "{% raw %}{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.{% endraw %}"
      summary: "{% raw %}Network interface is reporting many transmit errors.{% endraw %}"
    expr: "{% raw %}increase(node_network_transmit_errs_total{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[2m]) > 10{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeHighNumberConntrackEntriesUsed
    annotations:
      description: "{% raw %}{{ $value | humanizePercentage }} of conntrack entries are used{% endraw %}"
      summary: "{% raw %}Number of conntrack are getting close to the limit{% endraw %}"
    expr: "{% raw %}(node_nf_conntrack_entries{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_nf_conntrack_entries_limit{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) > 0.75{% endraw %}"
    labels:
      severity: warning
  - alert: NodeClockSkewDetected
    annotations:
      message: "{% raw %}Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure
        NTP is configured correctly on this host.{% endraw %}"
      summary: "{% raw %}Clock skew detected.{% endraw %}"
    expr: "{% raw %}(\n  node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0.05\nand\n  deriv(node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])\
      \ >= 0\n)\nor\n(\n  node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} < -0.05\nand\n  deriv(node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])\
      \ <= 0\n)\n{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: NodeClockNotSynchronising
    annotations:
      message: "{% raw %}Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured
        on this host.{% endraw %}"
      summary: "{% raw %}Clock not synchronising.{% endraw %}"
    expr: "{% raw %}min_over_time(node_timex_sync_status{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m]) == 0{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: CriticalCPULoad
    expr: "{% raw %}100 - (avg by (instance) (irate(node_cpu_seconds_total{job=\"node\",mode=\"idle\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) * 100) > 96{% endraw %}"
    for: 2m
    labels:
      severity: critical
    annotations:
      description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has Critical CPU load for more than 2 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Critical CPU load{% endraw %}"
  - alert: CriticalRAMUsage
    expr: "{% raw %}(1 - ((node_memory_MemFree_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} + node_memory_Buffers_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} + node_memory_Cached_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) / node_memory_MemTotal_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"})) * 100 > 98{% endraw %}"
    for: 5m
    labels:
      severity: critical
    annotations:
      description: "{% raw %}{{ $labels.instance }} has Critical Memory Usage more than 5 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} has Critical Memory Usage{% endraw %}"
{% endif %}

{% if 'kubernetes' in client.products %}
{% for prometheus in client.prometheus_federation %}
{% if prometheus.kubernetes_hosted %}
- name: "{{ client.name }}-{{ prometheus.name }}-kubernetes-apps"
  rules:
  - alert: KubePodCrashLooping
    annotations:
      message: "{% raw %}Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
    expr: "{% raw %}rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[15m]) * 60 * 5 > 0{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: KubePodNotReady
    annotations:
      message: "{% raw %}Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
        state for longer than an hour.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
    expr: "{% raw %}sum by (namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\", phase=~\"Pending|Unknown\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) > 0{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: KubeDeploymentGenerationMismatch
    annotations:
      message: "{% raw %}Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
        }} does not match, this indicates that the Deployment has failed but has not
        been rolled back.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
    expr: "{% raw %}kube_deployment_status_observed_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeDeploymentReplicasMismatch
    annotations:
      message: "{% raw %}Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
        matched the expected number of replicas for longer than an hour.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
    expr: "{% raw %}kube_deployment_spec_replicas{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_deployment_status_replicas_available{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: KubeStatefulSetReplicasMismatch
    annotations:
      message: "{% raw %}StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not
        matched the expected number of replicas for longer than 15 minutes.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
    expr: "{% raw %}kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeStatefulSetGenerationMismatch
    annotations:
      message: "{% raw %}StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
        }} does not match, this indicates that the StatefulSet has failed but has
        not been rolled back.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
    expr: "{% raw %}kube_statefulset_status_observed_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeStatefulSetUpdateNotRolledOut
    annotations:
      message: "{% raw %}StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
        has not been rolled out.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
    expr: "{% raw %}max without (revision) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} ) * (kube_statefulset_replicas{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}){% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeDaemonSetRolloutStuck
    annotations:
      message: "{% raw %}Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
        }}/{{ $labels.daemonset }} are scheduled and ready.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
    expr: "{% raw %}kube_daemonset_status_number_ready{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 100{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeDaemonSetNotScheduled
    annotations:
      message: "{% raw %}{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are not scheduled.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
    expr: "{% raw %}kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: KubeDaemonSetMisScheduled
    annotations:
      message: "{% raw %}{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are running where they are not supposed to run.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
    expr: "{% raw %}kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: KubeCronJobRunning
    annotations:
      message: "{% raw %}CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
        than 1h to complete.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
    expr: "{% raw %}time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 3600{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubeJobCompletion
    annotations:
      message: "{% raw %}Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than
        one hour to complete.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
    expr: "{% raw %}kube_job_spec_completions{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} - kube_job_status_succeeded{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}  > 0{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubeJobFailed
    annotations:
      message: "{% raw %}Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
    expr: "{% raw %}kube_job_status_failed{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}  > 0{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubePodFrequentlyRestarting
    annotations:
      description: "{% raw %}Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
        }}) has restarted {{ printf \"%.2f\" $value }} times over the last 5 minutes.{% endraw %}"
    expr: "{% raw %}increase(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m]) > 2{% endraw %}"
    labels:
      severity: critical

- name: "{{ client.name }}-{{ prometheus.name }}-kubernetes-storage"
  rules:
  - alert: KubePersistentVolumeUsageCritical
    annotations:
      message: "{% raw %}The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }}
        in Namespace {{ $labels.namespace }} is only {{ printf \"%0.2f\" $value }}%
        free.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
    expr: "{% raw %}100 * kubelet_volume_stats_available_bytes{job=\"kubelet\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} < 3{% endraw %}"
    for: 1m
    labels:
      severity: critical
  - alert: KubePersistentVolumeFullInFourDays
    annotations:
      message: "{% raw %}Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
        }} in Namespace {{ $labels.namespace }} is expected to fill up within four
        days. Currently {{ printf \"%0.2f\" $value }}% is available.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
    expr: "{% raw %}100 * (kubelet_volume_stats_available_bytes{job=\"kubelet\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} ) < 15 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 4 * 24 * 3600) < 0{% endraw %}"
    for: 5m
    labels:
      severity: critical
  - alert: KubePersistentVolumeErrors
    annotations:
      message: "{% raw %}The persistent volume {{ $labels.persistentvolume }} has status {{
        $labels.phase }}.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
    expr: "{% raw %}kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0{% endraw %}"
    for: 5m
    labels:
      severity: critical

- name: "{{ client.name }}-{{ prometheus.name }}-kube-apiserver.rules"
  rules:
  - expr: "{% raw %}histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) without(instance, pod)) / 1e+06{% endraw %}"
    labels:
      quantile: "0.99"
    record: cluster_quantile:apiserver_request_latencies:histogram_quantile
  - expr: "{% raw %}histogram_quantile(0.9, sum(rate(apiserver_request_latencies_bucket{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) without(instance, pod)) / 1e+06{% endraw %}"
    labels:
      quantile: "0.9"
    record: cluster_quantile:apiserver_request_latencies:histogram_quantile
  - expr: "{% raw %}histogram_quantile(0.5, sum(rate(apiserver_request_latencies_bucket{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) without(instance, pod)) / 1e+06{% endraw %}"
    labels:
      quantile: "0.5"
    record: cluster_quantile:apiserver_request_latencies:histogram_quantile

- name: "{{ client.name }}-{{ prometheus.name }}-kubernetes-system"
  rules:
  - alert: KubeNodeNotReady
    annotations:
      message: "{% raw %}{{ $labels.node }} has been unready for more than an hour.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
    expr: "{% raw %}kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubeVersionMismatch
    annotations:
      message: "{% raw %}There are {{ $value }} different semantic versions of Kubernetes components running.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
    expr: "{% raw %}count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~\"kube-dns|coredns\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"},\"gitVersion\",\"$1\",\"gitVersion\",\"(v[0-9]*.[0-9]*.[0-9]*).*\"))) > 1{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubeClientErrors
    annotations:
      message: "{% raw %}Kubernetes API server client {{ $labels.job }}/{{ $labels.instance }} is experiencing {{ printf \"%0.0f\" $value }}% errors.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
    expr: "{% raw %}(sum(rate(rest_client_requests_total{code=~\"5..\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (instance, job) / sum(rate(rest_client_requests_total{clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (instance, job)) * 100 > 1{% endraw %}"
    for: 15m
    labels:
      severity: warning
  - alert: KubeClientErrors
    annotations:
      message: "{% raw %}Kubernetes API server client {{ $labels.job }}/{{ $labels.instance }} is experiencing {{ printf \"%0.0f\" $value }} errors / second.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
    expr: "{% raw %}sum(rate(ksm_scrape_error_total{job=\"kube-state-metrics\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (instance, job) > 0.1{% endraw %}"
    for: 15m
    labels:
      severity: warning
  - alert: KubeletTooManyPods
    annotations:
      message: "{% raw %}Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 110.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
    expr: "{% raw %}kubelet_running_pod_count{job=\"kubelet\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 110 * 0.9{% endraw %}"
    for: 15m
    labels:
      severity: warning
  - alert: KubeAPILatencyHigh
    annotations:
      message: "{% raw %}The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
    expr: "{% raw %}cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 1{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: KubeAPILatencyHigh
    annotations:
      message: "{% raw %}The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
    expr: "{% raw %}cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 4{% endraw %}"
    for: 10m
    labels:
      severity: critical
  - alert: KubeAPIErrorsHigh
    annotations:
      message: "{% raw %}API server is returning errors for {{ $value }}% of requests.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
    expr: "{% raw %}sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) / sum(rate(apiserver_request_count{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) * 100 > 3{% endraw %}"
    for: 10m
    labels:
      severity: critical
  - alert: KubeAPIErrorsHigh
    annotations:
      message: "{% raw %}API server is returning errors for {{ $value }}% of requests.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
    expr: "{% raw %}sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) / sum(rate(apiserver_request_count{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) * 100 > 1{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: KubeAPIErrorsHigh
    annotations:
      message: "{% raw %}API server is returning errors for {{ $value }}% of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
    expr: "{% raw %}sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (resource,subresource,verb) / sum(rate(apiserver_request_count{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (resource,subresource,verb) * 100 > 10{% endraw %}"
    for: 10m
    labels:
      severity: critical
  - alert: KubeAPIErrorsHigh
    annotations:
      message: "{% raw %}API server is returning errors for {{ $value }}% of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
    expr: "{% raw %}sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (resource,subresource,verb) / sum(rate(apiserver_request_count{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (resource,subresource,verb) * 100 > 5{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: KubeClientCertificateExpiration
    annotations:
      message: "{% raw %}A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
    expr: "{% raw %}apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m]))) < 604800{% endraw %}"
    labels:
      severity: warning
  - alert: KubeClientCertificateExpiration
    annotations:
      message: "{% raw %}A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
    expr: "{% raw %}apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m]))) < 86400{% endraw %}"
    labels:
      severity: critical

- name: "{{ client.name }}-{{ prometheus.name }}-k8s.rules"
  rules:
  - expr: "{% raw %}sum(rate(container_cpu_usage_seconds_total{job=\"kubernetes-nodes-cadvisor\", image!=\"\", name!=\"\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (namespace){% endraw %}"
    record: namespace:container_cpu_usage_seconds_total:sum_rate
  - expr: "{% raw %}sum(container_memory_usage_bytes{job=\"kubernetes-nodes-cadvisor\", image!=\"\", name!=\"\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) by (namespace){% endraw %}"
    record: namespace:container_memory_usage_bytes:sum
  - expr: "{% raw %}sum by (namespace, pod_name, container_name) (rate(container_cpu_usage_seconds_total{job=\"kubernetes-nodes-cadvisor\", image!=\"\", name!=\"\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])){% endraw %}"
    record: namespace_pod_name_container_name:container_cpu_usage_seconds_total:sum_rate
  - expr: "{% raw %}sum by(namespace) (kube_pod_container_resource_requests_memory_bytes{job=\"kube-state-metrics\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~\"^(Pending|Running)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 1)){% endraw %}"
    record: namespace_name:kube_pod_container_resource_requests_memory_bytes:sum
  - expr: "{% raw %}sum by (namespace) (kube_pod_container_resource_requests_cpu_cores{job=\"kube-state-metrics\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~\"^(Pending|Running)$\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 1)){% endraw %}"
    record: namespace_name:kube_pod_container_resource_requests_cpu_cores:sum
  - expr: "{% raw %}sum(label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"ReplicaSet\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}, \"replicaset\", \"$1\", \"owner_name\", \"(.*)\") * on(replicaset, namespace) group_left(owner_name) kube_replicaset_owner{job=\"kube-state-metrics\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"},\"workload\", \"$1\", \"owner_name\", \"(.*)\")) by (namespace, workload, pod){% endraw %}"
    labels:
      workload_type: deployment
    record: mixin_pod_workload
  - expr: "{% raw %}sum(label_replace(kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"DaemonSet\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\")) by (namespace, workload, pod){% endraw %}"
    labels:
      workload_type: daemonset
    record: mixin_pod_workload
  - expr: "{% raw %}sum(label_replace(kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"StatefulSet\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\")) by (namespace, workload, pod){% endraw %}"
    labels:
      workload_type: statefulset
    record: mixin_pod_workload

- name: "{{ client.name }}-{{ prometheus.name }}-kubernetes-resources"
  rules:
  - alert: KubeCPUOvercommitPod
    annotations:
      message: Cluster has overcommitted CPU resource requests for Pods and cannot
        tolerate node failure.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
    expr: "{% raw %}sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum) / sum(node:node_num_cpu:sum) > (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum){% endraw %}"
    for: 5m
    labels:
      severity: warning
  - alert: KubeMemOvercommitPod
    annotations:
      message: Cluster has overcommitted memory resource requests for Pods and cannot
        tolerate node failure.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
    expr: "{% raw %}sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum) / sum(node_memory_MemTotal_bytes) > (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum){% endraw %}"
    for: 5m
    labels:
      severity: warning
  - alert: KubeCPUOvercommitNamespace
    annotations:
      message: Cluster has overcommitted CPU resource requests for Namespaces.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
    expr: "{% raw %}sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"cpu\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) / sum(node:node_num_cpu:sum) > 1.5{% endraw %}"
    for: 5m
    labels:
      severity: warning
  - alert: KubeMemOvercommitNamespace
    annotations:
      message: Cluster has overcommitted memory resource requests for Namespaces.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
    expr: "{% raw %}sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"memory\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) / sum(node_memory_MemTotal_bytes{job=\"node-exporter\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) > 1.5{% endraw %}"
    for: 5m
    labels:
      severity: warning
  - alert: KubeQuotaExceeded
    annotations:
      message: "{% raw %}Namespace {{ $labels.namespace }} is using {{ printf \"%0.0f\" $value
        }}% of its {{ $labels.resource }} quota.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
    expr: "{% raw %}100 * kube_resourcequota{job=\"kube-state-metrics\", type=\"used\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0) > 90{% endraw %}"
    for: 15m
    labels:
      severity: warning
  - alert: CPUThrottlingHigh
    annotations:
      message: "{% raw %}{{ printf \"%0.0f\" $value }}% throttling of CPU in namespace {{ $labels.namespace
        }} for container {{ $labels.container_name }} in pod {{ $labels.pod_name }}.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
    expr: "{% raw %}100 * sum(increase(container_cpu_cfs_throttled_periods_total{container_name!=\"\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (container_name, pod_name, namespace) / sum(increase(container_cpu_cfs_periods_total{clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) by (container_name, pod_name, namespace)\n  > 25 \n{% endraw %}"
    for: 15m
    labels:
      severity: warning
{% endif %}
{% endfor %}
{% endif %}

{% if 'elasticsearch' in client.products %}
- name: "{{ client.name }}-elasticsearch"
  rules:
  - alert: ElasticsearchClusterHealthStatus
    annotations:
      description: '{% raw %}Cluster {{ $labels.cluster }} of job {{ $labels.job }} has a non OK state for
        more than 5 minutes.{% endraw %}'
      summary: "{% raw %}Cluster {{ $labels.cluster }} not GREEN{% endraw %}"
    expr: '{% raw %}elasticsearch_cluster_health_status{color="green",clientID="{% endraw %}{{ client.name }}{% raw %}"} == 0{% endraw %}'
    for: 5m
    labels:
      severity: critical
  - alert: ElasticsearchDiskWaterMarkMinor
    annotations:
      description: '{% raw %}Raises when the elasticsearch instance {{ $labels.host }} uses 60% of disk space on the cluster {{ $labels.cluster }}.{% endraw %}'
      summary: "{% raw %}The elasticsearch instance {{ $labels.host }} uses 60% of disk space on {{ $labels.cluster }} cluster.{% endraw %}"
    expr: '{% raw %}(max by(host, instance) (elasticsearch_filesystem_data_size_bytes{clientID="{% endraw %}{{ client.name }}{% raw %}"}) - max by(host, instance) (elasticsearch_filesystem_data_available_bytes{clientID="{% endraw %}{{ client.name }}{% raw %}"})) / max by(host, instance) (elasticsearch_filesystem_data_size_bytes{clientID="{% endraw %}{{ client.name }}{% raw %}"}) >= 0.6{% endraw %}'
    for: 5m
    labels:
      severity: warning
  - alert: ElasticsearchDiskWaterMarkMinor
    annotations:
      description: '{% raw %}Raises when the elasticsearch instance {{ $labels.host }} uses 90% of disk space on the cluster {{ $labels.cluster }}.{% endraw %}'
      summary: "{% raw %}The elasticsearch instance {{ $labels.host }} uses 90% of disk space on {{ $labels.cluster }} cluster.{% endraw %}"
    expr: '{% raw %}(max by(host, instance) (elasticsearch_filesystem_data_size_bytes{clientID="{% endraw %}{{ client.name }}{% raw %}"}) - max by(host, instance) (elasticsearch_filesystem_data_available_bytes{clientID="{% endraw %}{{ client.name }}{% raw %}"})) / max by(host, instance) (elasticsearch_filesystem_data_size_bytes{clientID="{% endraw %}{{ client.name }}{% raw %}"}) >= 0.9{% endraw %}'
    for: 5m
    labels:
      severity: critical
{% endif %}

{% if 'apache' in client.products %}
- name: "{{ client.name }}-apache"
  rules:
  - alert: ApacheDown
    annotations:
      description: '{% raw %}Apache down on {{ $labels.instance }}. Job {{ $labels.job }}.{% endraw %}'
      summary: "{% raw %}Apache down (instance {{ $labels.instance }}){% endraw %}"
    expr: '{% raw %}apache_up{clientID="{% endraw %}{{ client.name }}{% raw %}"} == 0{% endraw %}'
    for: 0m
    labels:
      severity: critical
  - alert: ApacheRestart
    expr: '{% raw %}apache_uptime_seconds_total{clientID="{% endraw %}{{ client.name }}{% raw %}"} / 60 < 1{% endraw %}'
    for: 0m
    labels:
      severity: warning
    annotations:
      summary: "{% raw %}Apache restart (instance {{ $labels.instance }}){% endraw %}"
      description: "{% raw %}Apache has just been restarted on instance {{ $labels.instance }}.{% endraw %}"
  - alert: ApacheWorkersLoad
    expr: '{% raw %}(sum by (instance) (apache_workers{state="busy",clientID="{% endraw %}{{ client.name }}{% raw %}"}) / sum by (instance) (apache_scoreboard{clientID="{% endraw %}{{ client.name }}{% raw %}"}) ) * 100 > 80{% endraw %}'
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "{% raw %}Apache workers load (instance {{ $labels.instance }}){% endraw %}"
      description: "{% raw %}Apache workers in busy state approach the max workers count 80% workers busy on {{ $labels.instance }}{% endraw %}"
  - alert: ApacheFileDescriptor
    expr: '{% raw %}(sum by (instance) (process_open_fds{clientID="{% endraw %}{{ client.name }}{% raw %}",job="apache-exporter"}) / sum by (instance) (process_max_fds{clientID="{% endraw %}{{ client.name }}{% raw %}",job="apache-exporter"}) ) * 100 > 80{% endraw %}'
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "{% raw %}Apache file descriptor usage (instance {{ $labels.instance }}){% endraw %}"
      description: "{% raw %}Apache used file descriptor approach 80% of the max file descriptor on {{ $labels.instance }}.{% endraw %}"
{% endif %}
