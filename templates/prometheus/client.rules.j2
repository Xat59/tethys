groups:
{% if 'node' in client.products %}
- name: "{{ client.name }}-node"
  rules:
  - alert: InstanceDown
    annotations:
      description: '{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for
        more than 5 minutes.{% endraw %}'
      summary: "{% raw %}Instance {{ $labels.instance }} down{% endraw %}"
    expr: '{% raw %}up{clientID="{% endraw %}{{ client.name }}{% raw %}"} == 0{% endraw %}'
    for: 5m
    labels:
      severity: critical
  - alert: RebootRequired
    annotations:
      description: "{% raw %}{{ $labels.instance }} requires a reboot.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - reboot required{% endraw %}"
    expr: "{% raw %}node_reboot_required{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0{% endraw %}"
    labels:
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available space left and is filling up.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of space within the next 24 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 40\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 24*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of space within the next 4 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 20\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 4*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
        node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available space left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 5% space left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 5\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
    \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
          only {{ printf \"%.2f\" $value }}% available space left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 3% space left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_size_bytes{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 3\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
    \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of inodes within the next 24 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 40\nand\n  predict_linear(node_filesystem_files_free{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 24*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
    node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.{% endraw %}"
      summary: "{% raw %}Filesystem is predicted to run out of inodes within the next 4 hours.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 20\nand\n  predict_linear(node_filesystem_files_free{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[6h], 4*60*60) < 0\nand\n  node_filesystem_readonly{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 5% inodes left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 5\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
      \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: "{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
        only {{ printf \"%.2f\" $value }}% available inodes left.{% endraw %}"
      summary: "{% raw %}Filesystem has less than 3% inodes left.{% endraw %}"
    expr: "{% raw %}(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_filesystem_files{job=\"\
      node\",fstype!=\"\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 3\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\
      \",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} == 0\n)\n{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: NodeNetworkReceiveErrs
    annotations:
      description: "{% raw %}{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf \"%.0f\" $value }} receive errors in the last two minutes.{% endraw %}"
      summary: "{% raw %}Network interface is reporting many receive errors.{% endraw %}"
    expr: "{% raw %}increase(node_network_receive_errs_total{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[2m]) > 10{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeNetworkTransmitErrs
    annotations:
      description: "{% raw %}{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.{% endraw %}"
      summary: "{% raw %}Network interface is reporting many transmit errors.{% endraw %}"
    expr: "{% raw %}increase(node_network_transmit_errs_total{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[2m]) > 10{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: NodeHighNumberConntrackEntriesUsed
    annotations:
      description: "{% raw %}{{ $value | humanizePercentage }} of conntrack entries are used{% endraw %}"
      summary: "{% raw %}Number of conntrack are getting close to the limit{% endraw %}"
    expr: "{% raw %}(node_nf_conntrack_entries{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / node_nf_conntrack_entries_limit{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) > 0.75{% endraw %}"
    labels:
      severity: warning
  - alert: NodeClockSkewDetected
    annotations:
      message: "{% raw %}Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure
        NTP is configured correctly on this host.{% endraw %}"
      summary: "{% raw %}Clock skew detected.{% endraw %}"
    expr: "{% raw %}(\n  node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0.05\nand\n  deriv(node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])\
      \ >= 0\n)\nor\n(\n  node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} < -0.05\nand\n  deriv(node_timex_offset_seconds{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])\
      \ <= 0\n)\n{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: NodeClockNotSynchronising
    annotations:
      message: "{% raw %}Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured
        on this host.{% endraw %}"
      summary: "{% raw %}Clock not synchronising.{% endraw %}"
    expr: "{% raw %}min_over_time(node_timex_sync_status{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m]) == 0{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: CriticalCPULoad
    expr: "{% raw %}100 - (avg by (instance) (irate(node_cpu_seconds_total{job=\"node\",mode=\"idle\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m])) * 100) > 96{% endraw %}"
    for: 2m
    labels:
      severity: critical
    annotations:
      description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has Critical CPU load for more than 2 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Critical CPU load{% endraw %}"
  - alert: CriticalRAMUsage
    expr: "{% raw %}(1 - ((node_memory_MemFree_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} + node_memory_Buffers_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} + node_memory_Cached_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) / node_memory_MemTotal_bytes{clientID=\"{% endraw %}{{ client.name }}{% raw %}\"})) * 100 > 98{% endraw %}"
    for: 5m
    labels:
      severity: critical
    annotations:
      description: "{% raw %}{{ $labels.instance }} has Critical Memory Usage more than 5 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} has Critical Memory Usage{% endraw %}"
{% endif %}

{% if 'kubernetes' in client.products %}
{% for prometheus in client.prometheus_federation %}
{% if prometheus.kubernetes_hosted %}
- name: "{{ client.name }}-{{ prometheus.name }}-kubernetes-apps"
  rules:
  - alert: KubePodCrashLooping
    annotations:
      message: "{% raw %}Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
    expr: "{% raw %}rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\",clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\",clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[15m]) * 60 * 5 > 0{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: KubePodNotReady
    annotations:
      message: "{% raw %}Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
        state for longer than an hour.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
    expr: "{% raw %}sum by (namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\", phase=~\"Pending|Unknown\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}) > 0{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: KubeDeploymentGenerationMismatch
    annotations:
      message: "{% raw %}Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
        }} does not match, this indicates that the Deployment has failed but has not
        been rolled back.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
    expr: "{% raw %}kube_deployment_status_observed_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeDeploymentReplicasMismatch
    annotations:
      message: "{% raw %}Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
        matched the expected number of replicas for longer than an hour.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
    expr: "{% raw %}kube_deployment_spec_replicas{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_deployment_status_replicas_available{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 1h
    labels:
      severity: critical
  - alert: KubeStatefulSetReplicasMismatch
    annotations:
      message: "{% raw %}StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not
        matched the expected number of replicas for longer than 15 minutes.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
    expr: "{% raw %}kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeStatefulSetGenerationMismatch
    annotations:
      message: "{% raw %}StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
        }} does not match, this indicates that the StatefulSet has failed but has
        not been rolled back.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
    expr: "{% raw %}kube_statefulset_status_observed_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeStatefulSetUpdateNotRolledOut
    annotations:
      message: "{% raw %}StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
        has not been rolled out.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
    expr: "{% raw %}max without (revision) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} ) * (kube_statefulset_replicas{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}){% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeDaemonSetRolloutStuck
    annotations:
      message: "{% raw %}Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
        }}/{{ $labels.daemonset }} are scheduled and ready.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
    expr: "{% raw %}kube_daemonset_status_number_ready{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} / kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} * 100 < 100{% endraw %}"
    for: 15m
    labels:
      severity: critical
  - alert: KubeDaemonSetNotScheduled
    annotations:
      message: "{% raw %}{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are not scheduled.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
    expr: "{% raw %}kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: KubeDaemonSetMisScheduled
    annotations:
      message: "{% raw %}{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are running where they are not supposed to run.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
    expr: "{% raw %}kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 0{% endraw %}"
    for: 10m
    labels:
      severity: warning
  - alert: KubeCronJobRunning
    annotations:
      message: "{% raw %}CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
        than 1h to complete.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
    expr: "{% raw %}time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} > 3600{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubeJobCompletion
    annotations:
      message: "{% raw %}Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than
        one hour to complete.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
    expr: "{% raw %}kube_job_spec_completions{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"} - kube_job_status_succeeded{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}  > 0{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubeJobFailed
    annotations:
      message: "{% raw %}Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.{% endraw %}"
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
    expr: "{% raw %}kube_job_status_failed{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}  > 0{% endraw %}"
    for: 1h
    labels:
      severity: warning
  - alert: KubePodFrequentlyRestarting
    annotations:
      description: "{% raw %}Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
        }}) has restarted {{ printf \"%.2f\" $value }} times over the last 5 minutes.{% endraw %}"
    expr: "{% raw %}increase(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\", clusterID=\"{% endraw %}{{ prometheus.name }}{% raw %}\", clientID=\"{% endraw %}{{ client.name }}{% raw %}\"}[5m]) > 2{% endraw %}"
    labels:
      severity: critical
{% endif %}
{% endfor %}
{% endif %}
